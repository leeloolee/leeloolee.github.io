나의 구현 포스팅은 절반은 베껴온거

이번엔 그 유명한 닼프한테서 베껴온거

https://darkpgmr.tistory.com/110





주성분 분석, 영어로는 PCA(Principal Component Analysis)



주성분 분석(PCA)은 사람들에게 비교적 널리 알려져 있는 방법으로서, 다른 블로그, 카페 등에 이와 관련된 소개글 또한 굉장히 많다.



그래도 기존에 이미 있는 내용들과 차별성이 있다면 이 글은 주성분 분석(PCA)을 자신의 공부, 연구 또는 개발에 보다 잘 활용할 수 있도록 주성분 분석(PCA)의 다양한 활용 예를 중심으로 기본 원리 등을 가급적 폭넓게 다뤄보고자 한다. 



주성분 분석(PCA)은 사실 선형대수학이라기 보다는 선형대수학의 활용적인 측면이 강하며 영상인식, 통계 데이터 분석(주성분 찾기), 데이터 압축(차원 감소), 노이즈 제거 등 다양한 활용을 갖는다.



PCA(Principal component analysis)에 대한 계산 방법이나 이론적인 부분은 뒤에 가서 다루고 일단은 PCA에 대한 개념 및 활용적인 측면을 살펴보도록 하자. 



1. PCA(Principal Component Analysis)란

PCA는 분포된 데이터들의 주성분(Principal Component)를 찾아주는 방법이다. 

좀 더 구체적으로 보면 아래 그림과 같이 2차원 좌표평면에 n개의 점 데이터 (x1, y1), (x2, y2), ... , (xn, yn)들이 타원형으로 분포되어 있을 때

<img src="https://t1.daumcdn.net/cfile/tistory/25388D40527C43DB0B" alt="img" style="zoom:50%;" />

이 데이터들의 분포 특성을 2개의 벡터로 가장 잘 설명할 수 있는 방법은 무엇일까?

그건 바로, 그림에서와 같이 **e1, e2 두 개의 벡터로 데이터 분포를 설명**하는 것이다. 

e1의 방향과 크기, 그리고 e2 방향과 크기를 알면 이 데이터 분포가 어떤 형태인지를 가장 단순하면서도 효과적으로 파악할 수 있다. 

즉, PCA에서 하고자 하는 것은 e1, e2 두개의 벡터를 찾고자 하는 것이다. 



PCA는 데이터 하나 하나에 대한 성분을 분석하는 것이 아니라, 여러 데이터들이 모여 하나의 분포를 이룰 때 이 분포의 주 성분을 분석해 주는 방법이다.



여기서 주성분이라 함은 그 방향으로 데이터들의 분산이 가장 큰 방향벡터를 의미한다. 

<그림 1>에서 e1 방향을 따라 데이터들의 분산(흩어진 정도)이 가장 크다. 

그리고 e1에 수직이면서 그 다음으로 데이터들의 분산이 가장 큰 방향은 e2이다.



PCA는 2차원 데이터 집합에 대해 PCA를 수행하면 2개의 서로 수직인 주성분 벡터를 반환하고, 3차원 점들에 대해 PCA를 수행하면 3개의 서로 수직인 주성분 벡터들을 반환한다. 

예를 들어 3차원 데이터의 경우는 아래 그림과 같이 3개의 서로 수직인 주성분 벡터를 찾아준다. 



<img src="https://t1.daumcdn.net/cfile/tistory/232FCB42527C51481B" alt="img" style="zoom:50%;" />





2. PCA (Principal Component Analysis) 활용



A. 2D 점들의 직선 근사



이전 글 [선형대수학 #5] 선형연립방정식 풀이에서 예로 들었던 네 점 (1,3.5), (2,4.3), (3, 7.2)

), (4,8)을 가장 잘 근사하는 직선의 방정식을 PCA로 구해보자

<img src="https://t1.daumcdn.net/cfile/tistory/2728A944527C5A9C0E" alt="img" style="zoom:50%;" />

PCA로 직선을 근사하는 방법은 데이터들의 평균 위치를 지나면서 PCA로 나온 제 1주성분 벡터와 평행인 직선을 구하면 된다. 

실제로 위 네 점을 지나는 직선을 PCA로 구해보면 y = 1.7194x + 1.4515가 나오며 그 그래프는 <그림 3>과 같다. 그림에서 보듯이 PCA로 구한 직선과 최소자승법(LS 방법)으로 구한 직선이 모두 다름을 볼 수 있다. 그 이유는 최소자승법은 직선과 데이터와의 거리를 최소화하는 반면 PCA는 데이터의 분산이 가장 큰 방향을 구하기 때문이다. 



그림 3에서 LS근사(y= ax+b)는 직선과의 y축 거리를 최소화시키고, LS 근사(ax+by+c=0)는 평면 z = ax +by+c와의 z축 거리를 최소화시킨다. (평면 z = ax+by+c과 xy 평면과의 교선이 ax+by+c=0). PCA는 데이터들의 평균점을 지나는 직선들 중에서 데이터들을 직선에 투영(projection)시켰을 때 해당 직선을 따라서 데이터의 분산이 최대가 되는 방향의 직선을 구한다. 



B. 3D 점들의 직선 또는 평면 근사



3차원 공간에서 점들의 집합을 직선으로 근사하는 문제는 최소자승법(LS, Least square Method)으로는 쉽지 않다. 하지만 주성분분석(PCA)을 활용하면 이를 손 쉽고 효율적으로 구할 수 있다. 



만일 주어진 데이터 점 (x1, y1, z1), (x2, y2, z2), ... (xn, yn, zn)들이 평균이 (mx, my, mz)이고 PCA로 구한 제 1주성분 벡터가 e1 = (a,b,c)라면 이들 데이터 점들을 근사하는 직선식은 다음과 같다. 

![img](https://t1.daumcdn.net/cfile/tistory/2727A23E527C5FF52E)

이들 점들을 평면 방정식으로 근사하는 경우에도 PCA를 적용할 수 있다. 

만일 PCA로 나온 제 1 주성분 벡터가 e1, 제 2 주성분 벡터가 e2라면 근사 평면 방정식은 다음과 같다. 
$$
(e_1 \times e_2) \cdot (x-m) = 0  
$$
단, x는 벡터의 외적, x= (x,y,z), m = (mx, my, mz)



C. eigenface와 영상 인식 응용

PCA가 영상 인식에 활용되는 대표적인 예는 얼굴 인식(face recognition)이다. 그리고 이와 관련된 개념 혹은 용어로서 (eigenface)라는 게 있다. 



다음과 같은 20개의 45×40 얼굴 이미지들이 있다고 하자. 

![img](https://t1.daumcdn.net/cfile/tistory/26178F50527C89A126)

이미지에서 픽셀 밝기 값을 일렬로 연결하여 벡터로 만들면 이들 각각의 얼굴 이미지는 45×40  = 1800 차원의 벡터로 생각할 수 있다. (즉, 각각의 이미지는 1,800 차원 공간에서 한 점(좌표)에 대응)



이제 이 20개의 1,800차원 점 데이터들을 가지고 PCA를 수행하면 데이터의 차원 수와 동일한 개수의 주성분 벡터들을 얻을 수 있다. 

이렇게 얻어진 주성분 벡터들을 다시 이미지로 해석한 것이 eigenface이다 (얼굴 이미지를 가지고 얻은 벡터이기에 eigenface라 부른다.)

실제 위 이미지에  대해 얻어진 1







3. PCA의 계산

PCA를 알기 위해서는 먼저 공분산 행렬(covariance matrix)에 대해 알아야 한다. 



먼저, x와 y의 공분산(covariance)이란 아래 식과 같이 정의된다. 


$$
cov(x,y) = E[(X-m_x)(y-m_x)] \\
		  = E[xy]- m_xm_y
$$
단, mx는 x의 평균, my는 평균, E[]는 기대값(평균)

x의 분산은 x들이 평균을 중심으로 얼마나 흩어져 있는지를 나타내고, x와 y의 공분산은 x,y의 흩어진 정도가 얼마나 서로 상관관계를 가지고 흩어졌는지를 나타낸다. 

예를 들어, x와 y 각각의 분산은 일정한데 x가 mx보다 클 때 y도 my보다 크면 공분산은 최대가 되고, x가 mx보다 커질 때 y는 my보다 작아지면 공분산은 최소(음수가 됨), 서로 상관관계가 없으면 공분산은 0이 된다. 



공분산 행렬(covariance matrix)이란 데이터의 좌표 성분들 사이의 공분산 값을 원소로 하는 행렬로서 데이터의 i번째 좌표 성분과 j번째 좌표 성분의 공분산 값을 행렬의 i행 j열 원소값으로 하는 행렬이다. 

<img src="https://t1.daumcdn.net/cfile/tistory/2369DC4A527CCE7E21" alt="img" style="zoom:50%;" />

예를 들어, 2차원 데이터 n개가 (x1, y1), (x2, y2), ..., (xn, yn)와 같이 있다면 이 데이터들의 공분산 행렬은 다음과 같이 계산된다. 

<img src="https://t1.daumcdn.net/cfile/tistory/250A2D36527CD00E21" alt="img" style="zoom:50%;" />

PCA란 한마디로 말하면 입력 데이터들의 공분산 행렬(covariance matrix)에 대한 고유값 분해(eigendecomposition)로 볼 수 있다(고유값 분해에 대해서는 [선형대수학 #3] 고유값과 고유벡터 글 참조) 이 때 나오는 고유벡터가 주성분 벡터로서 데이터의 분포에서 분산이 큰 방향을 나타내고, 대응되는 고유값(eigenvalue)이 그 분산의 크기를 나타낸다. 

![img](https://t1.daumcdn.net/cfile/tistory/233FDA4C527CC7B728)





PCA 수도코드

Real-valued training sample of size m

mean centered data matrix 
$$
{X \in R^{n\times m}
\\
sample\ covariance\ matrix\ C = 1/m XX^T
\\
matrix\ of\ top\ d\ eigenvecors\ of\ C\ is\ U \in R^{m\times d}
\\
PCA\ of\ x\ is\ U^TX}
$$
